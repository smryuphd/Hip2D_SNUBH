{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10038f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40107dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # GPU 및 라이브러리 불러오기\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# !nvidia-smi\n",
    "import os\n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# import pydicom\n",
    "# import nibabel as nib\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, natsort\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import monai\n",
    "from monai.networks.utils import one_hot\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c0873-5331-4495-88e6-a00d0d5d29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88efba8a-feba-495c-be6b-29b2074bc148",
   "metadata": {},
   "source": [
    "## Dataloader 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b59318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train = natsort.natsorted(glob.glob('preprocessed/processed_230107_nl/Train/*.png')) + natsort.natsorted(glob.glob('preprocessed/processed_230107_abnl/Train/*.png'))\n",
    "x_test = natsort.natsorted(glob.glob('preprocessed/processed_230107_nl/Test/*.png')) + natsort.natsorted(glob.glob('preprocessed/processed_230107_abnl/Test/*.png')) \n",
    "\n",
    "x_train, x_valid = train_test_split(x_train,test_size=0.2, random_state=42, shuffle=True)\n",
    "print(len(x_train), len(x_valid), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3a5dd-f8cc-4ac2-ba2d-0fdba01cd40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "322c60f9-620a-4cde-95ed-999fd1846f10",
   "metadata": {
    "tags": []
   },
   "source": [
    "## augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b5a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "transform_train = A.Compose([\n",
    "    A.CenterCrop(height=800,width=1024,p=1),\n",
    "    A.PadIfNeeded(min_height=1024,min_width=1024,border_mode=cv2.BORDER_CONSTANT,p=1),\n",
    "    A.Resize(height=512, width=512, interpolation=cv2.INTER_CUBIC,p=1), #다시 512로\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.1),\n",
    "\n",
    "    A.OneOf([\n",
    "    A.InvertImg(p=0.5),\n",
    "    A.ChannelShuffle(p=.5),\n",
    "    ],p=0.2),\n",
    "\n",
    "    A.OneOf([\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=False, p=0.5),\n",
    "    A.RandomGamma(gamma_limit=(80,120), p=.5),\n",
    "    A.RandomToneCurve(scale=0.4 ,p=.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=.5),\n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=.5),\n",
    "    ],p=0.5),\n",
    "\n",
    "    A.OneOf([\n",
    "    A.MotionBlur(blur_limit=7, p=0.5),\n",
    "    A.MedianBlur(blur_limit=7, p=0.5),\n",
    "    A.GlassBlur(sigma=0.3, max_delta=2, p=0.5),\n",
    "    A.Sharpen(alpha=(0.1, 0.3), lightness=(0.7, 1.1), p=0.5)\n",
    "    ],p=0.2),\n",
    "\n",
    "    A.OneOf([\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), mean=0, p=0.5),\n",
    "    A.MultiplicativeNoise(multiplier=(0.98, 1.02), p=0.5),\n",
    "    A.ISONoise(color_shift=(0.01, 0.02), intensity=(0.1, 0.3), p=0.5),\n",
    "    ],p=0.3),\n",
    "\n",
    "    A.OneOf([\n",
    "    A.ElasticTransform(border_mode=cv2.BORDER_CONSTANT, interpolation=cv2.INTER_CUBIC, alpha=1, sigma=50, alpha_affine=50, p=0.5),\n",
    "    A.GridDistortion(border_mode=cv2.BORDER_CONSTANT, interpolation=cv2.INTER_CUBIC, distort_limit=0.3, num_steps=5, p=0.5),\n",
    "    A.OpticalDistortion(border_mode=cv2.BORDER_CONSTANT, interpolation=cv2.INTER_CUBIC, distort_limit=.05, shift_limit=0.05, p=0.5),\n",
    "    ],p=0.5),\n",
    "    \n",
    "    A.ShiftScaleRotate(border_mode=cv2.BORDER_CONSTANT, interpolation=cv2.INTER_CUBIC, shift_limit=0.0625, scale_limit=0.0625, rotate_limit=20, p=0.5),\n",
    "])\n",
    "\n",
    "transform_valid = A.Compose([\n",
    "    A.CenterCrop(height=800,width=1024,p=1),\n",
    "    A.PadIfNeeded(min_height=1024,min_width=1024,border_mode=cv2.BORDER_CONSTANT,p=1),\n",
    "    A.Resize(height=512, width=512, interpolation=cv2.INTER_CUBIC,p=1) #다시 512로\n",
    "])\n",
    "\n",
    "\n",
    "import mclahe as mc\n",
    "class datasets():\n",
    "    \"\"\"\n",
    "    explanation\n",
    "    x_list:\n",
    "    y_list:\n",
    "    \"\"\"\n",
    "    def __init__(self, x_list, augmentation=None):\n",
    "        self.x_list= x_list\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # read data\n",
    "        path_x = self.x_list[idx]\n",
    "        x = cv2.imread(path_x) # H x W x 3    # 0 ~ 255   이게 3이라서 밑에 in_channel을 3으로 했다..\n",
    "        #x = cv2.imread(path_x, 0) # H x W  이 된다.. 하지만 3 channel로 돌아가는 augmentation이 많다.\n",
    "        x = cv2.cvtColor(x,cv2.COLOR_BGR2RGB) # H x W x 3 # 0 ~ 255\n",
    "        y = np.array([0]) if 'normal' in self.x_list[idx] else np.array([1]) # 0 for nl 1 for abnl\n",
    "        \n",
    "        # augmentation\n",
    "        if self.augmentation:\n",
    "            transformed = self.augmentation(image=x) # return is dictionary transformed   ['image'] --> x, transformed['mask']  - y  #이미지와 마스크를 pair로 넣어줘야 한다.\n",
    "            x = transformed['image']\n",
    "            \n",
    "        #normalization\n",
    "        # x = x/255. # 0 ~ 255 --> 0 ~ 1\n",
    "        x = mc.mclahe(x,(128,128,3))  #(128,128 정도의 patch로 순차적 normlization)\n",
    "        \n",
    "        # to torch type / 3 H W \n",
    "        x = np.moveaxis(x,-1,0).astype(np.float32)\n",
    "        \n",
    "        return {'x':x,'y':y, 'fname':path_x}\n",
    "    \n",
    "\n",
    "# 외부에서 따옴\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices: a list of indices\n",
    "        num_samples: number of samples to draw\n",
    "        callback_get_label: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices = None, num_samples = None, callback_get_label = None):\n",
    "        self.indices = list(range(len(dataset))) if indices is None else indices        # if indices is not provided, all elements in the dataset will be considered\n",
    "        self.callback_get_label = callback_get_label                                    # define custom callback\n",
    "        self.num_samples = len(self.indices) if num_samples is None else num_samples    # if num_samples is not provided, draw `len(indices)` samples in each iteration\n",
    "\n",
    "        df = pd.DataFrame()                                                             # distribution of classes in the dataset\n",
    "        \n",
    "        label = []\n",
    "        for idx in tqdm.tqdm(range(len(dataset))):\n",
    "            ########## customize here ###############\n",
    "            l = dataset[idx]['y'] # <-- return type of dataset was dictionary and 'y' was our label\n",
    "            if 1 in l:\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)                \n",
    "            ########## customize here ###############\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        df[\"label\"] = label\n",
    "        df.index = self.indices\n",
    "        df = df.sort_index()\n",
    "\n",
    "        label_to_count = df[\"label\"].value_counts()\n",
    "\n",
    "        weights = 1.0 / label_to_count[df[\"label\"]] # almost equally\n",
    "#         weights = 1.0 / (label_to_count[df[\"label\"]])**2 # slightly weighted to 1\n",
    "        self.weights = torch.DoubleTensor(weights.to_list())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59dff2-1894-4e98-a5dd-d9cef8df881f",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c60da-12cd-440c-ac9c-090a5ca64afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class dataset:\n",
    "    \n",
    "    def __init__(self, x_list, augmentation = False):\n",
    "        # variance initialization\n",
    "        self.x_list = x_list\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def __len__(self):\n",
    "        # give information of total dataset numbers\n",
    "        return len(self.x_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # data extraction\n",
    "        fname = self.x_list[idx]\n",
    "        x = cv2.imread(fname) # (1024, 1024, 3)\n",
    "        x = cv2.cvtColor(x,cv2.COLOR_BGR2RGB) # H x W x 3 # 0 ~ 255\n",
    "    \n",
    "        if np.percentile(x,90)>250:\n",
    "            x = 255 - x\n",
    "            \n",
    "        if 'abnl' in fname:\n",
    "            y = np.array([1]) # (1)\n",
    "        else: \n",
    "            y = np.array([0]) # (1)\n",
    "        \n",
    "        # augmentation by Albumentation\n",
    "        if self.augmentation:\n",
    "            transformed = self.augmentation(image=x) # return is dictionary transformed   \n",
    "            # ['image'] --> x, transformed['mask']  - y  #이미지와 마스크를 pair로 넣어줘야 한다.\n",
    "            x = transformed['image']\n",
    "            \n",
    "        x = mc.mclahe(x,(128,128,3))  #(128,128 정도의 patch로 순차적 normlization)\n",
    "        # numpy (H,W,C) --> torch (C,H,W)\n",
    "        # x = x/255 # x = x/255\n",
    "        x = np.moveaxis(x,-1,0)\n",
    "        x = torch.tensor(x)\n",
    "        \n",
    "        # return x, y\n",
    "        return {'x':x, 'y':y, 'fname':fname}\n",
    "\n",
    "train_dataset = dataset(x_train, transform_train)\n",
    "valid_dataset = dataset(x_valid, transform_valid)\n",
    "test_dataset = dataset(x_test, transform_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f80ca-479d-462d-8f9b-0b589c933838",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=24,shuffle=False, sampler=ImbalancedDatasetSampler(train_dataset), pin_memory=True) #Imbalance쓰려면 False해야!\n",
    "torch.save(train_loader, 'dataloader/Imbalanced_Trainloader_Hip_2C_240503.pt')\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=24,shuffle=False, sampler=ImbalancedDatasetSampler(valid_dataset), pin_memory=True)\n",
    "torch.save(valid_loader, 'dataloader/Imbalanced_Validloader_Hip_2C_240503.pt')\n",
    "test_loader = DataLoader(test_dataset,batch_size=4)\n",
    "\n",
    "print(len(train_loader), len(valid_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a80669-a9f3-4ac3-a277-033bcd052b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.load('dataloader/Imbalanced_Trainloader_Hip_2C_240503.pt')\n",
    "valid_loader = torch.load('dataloader/Imbalanced_Validloader_Hip_2C_240503.pt')\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "print(len(train_loader), len(valid_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598527e-d462-43be-bf6e-c827cd1fa494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1  #label을 만드는중 (정상0, 비정상1)  1106 277 / 346\n",
    "y_train = np.concatenate([np.zeros(400),np.ones(983)])   #0과 1로 된거 400개씩\n",
    "y_test = np.concatenate([np.zeros(100),np.ones(246)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e254a-99e0-4c02-ba28-246a41c10261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class dataset:\n",
    "    \n",
    "    def __init__(self, x_list, y_list, augmentation = False):\n",
    "        # variance initialization\n",
    "        self.x_list = x_list\n",
    "        self.y_list = y_list\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def __len__(self):\n",
    "        # give information of total dataset numbers\n",
    "        return len(self.x_list)  #x랑 y가 같을 거니까 x만 해줘도 된다.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # data extraction\n",
    "        fname = self.x_list[idx]\n",
    "        x = cv2.imread(fname) # (1024, 1024, 3) # 파일명을 numpy로\n",
    "#        y = self.y_list[idx]\n",
    "        y = np.array([self.y_list[idx]])\n",
    "        \n",
    "        # numpy (H,W,C) --> torch (C,H,W)\n",
    "        x = np.moveaxis(x,-1,0)\n",
    "        # x = torch.tensor(x)\n",
    "        \n",
    "        # return x, y\n",
    "        return {'x':x, 'y':y, 'fname':fname}\n",
    "\n",
    "train_dataset = dataset(x_train, y_train, transform_train)\n",
    "valid_dataset = dataset(x_valid, y_valid, transform_valid)\n",
    "test_dataset = dataset(x_test, y_test, transform_valid)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=2,shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=2,shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=2)\n",
    "\n",
    "print(len(train_loader), len(valid_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7f0b5-b472-4140-9292-ebbccfe6f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import pylab as plt\n",
    "# img = cv2.imread('processed_nl/040712_000000_11457954_normal.dcm.png')\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404116fc-4aa7-4ca7-b355-d89d6cbeb635",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))\n",
    "x = batch['x']\n",
    "y = batch['y']\n",
    "x.shape, y.shape  # B C H W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a691de-870a-4967-8c69-0ad254bc4369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa62002",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_dataset = datasets(x_train,transform_train_woCrop)\n",
    "valid_dataset = datasets(x_valid,transform_valid_woCrop)\n",
    "test_dataset = datasets(x_test,transform_valid_woCrop)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=4, pin_memory=True)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=False, batch_size=4, pin_memory=True, sampler=ImbalancedDatasetSampler(train_dataset))  # 매 배치당 샘플러에 정의된 비율로 들어가게 해준다.\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=2, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a77f1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81232309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import monai\n",
    "net = monai.networks.nets.EfficientNetBN(\"efficientnet-b3\", in_channels = 3, num_classes=1, spatial_dims = 2, norm='batch', pretrained=True, adv_prop=True)  #classification\n",
    "# net\n",
    "# # test model\n",
    "# x = torch.rand(2, 3, 1024,1024)\n",
    "# yhat = net(x)\n",
    "# yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94165008-b97e-43e8-bcf0-478cca067377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modules_smr.ArcFace import *\n",
    "from modules_smr.NLB import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb0842-f263-4283-b7c9-abb98e6e5417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import monai\n",
    "\n",
    "# nnblock = NLBlockND(in_channels=1024, dimension=2)\n",
    "# net.features.denseblock4 = nn.Sequential(net.features.denseblock4, nnblock)\n",
    "\n",
    "nnblock = NLBlockND(in_channels=net._conv_head.out_channels, dimension=2)\n",
    "net._conv_head = nn.Sequential(net._conv_head, nnblock)\n",
    "\n",
    "net = net.to(device)\n",
    "# net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b657ee15-53e9-49e4-9de9-dae701ae9242",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da8138-cb86-421d-ad87-ecba7640d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Accuracy\n",
    "train_accuracy = Accuracy(task='binary')\n",
    "valid_accuracy = Accuracy(task='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c37b1f-751b-4286-b86e-7f9e8fe35a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import monai\n",
    "from sklearn.metrics import *\n",
    "\n",
    "binarization = monai.transforms.AsDiscrete(threshold=0.5)\n",
    "\n",
    "def metrics(yhat,y):\n",
    "    \"\"\"\n",
    "    Binary classification metric\n",
    "    \n",
    "    input : long type inputs torch or numpy\n",
    "    output : various metric in dictionary form\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            yhat = yhat.flatten().cpu().detach().numpy()\n",
    "            y = y.flatten().cpu().detach().numpy()\n",
    "        except:\n",
    "            yhat = yhat.flatten().numpy()\n",
    "            y = y.flatten().numpy()\n",
    "    except:\n",
    "        yhat = yhat.flatten()\n",
    "        y = y.flatten()\n",
    "    \n",
    "    cm = confusion_matrix(y, yhat)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, yhat).ravel()   #sklearn #ravel: binary일때\n",
    "    accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "    iou = tp/(tp+fp+fn)\n",
    "    f1 = 2*tp/(2*tp+fp+fn)\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    ppv = tp / (tp+fp)\n",
    "    npv = tn / (tn+fn)\n",
    "#     print('cm',confusion_matrix(y, yhat))\n",
    "    \n",
    "    return {'accuracy':accuracy,\n",
    "            'f1':f1, \n",
    "            'iou':iou, \n",
    "            'npv':npv,\n",
    "            'sensitivity':sensitivity,\n",
    "            'specificity':specificity,\n",
    "            'ppv':ppv,\n",
    "            'TP':tp,\n",
    "            'FP':fp,\n",
    "            'FN':fn,\n",
    "            'TN':tn,\n",
    "            'cm':cm\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27d2d8-3883-45fc-93a4-401574a4cb4b",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "lossfn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=1e-3)\n",
    "\n",
    "def train(loader):\n",
    "    losses = []\n",
    "    net.train()\n",
    "    for idx, batch in tqdm.tqdm(enumerate(loader), desc='train', total=len(loader)):\n",
    "        x = batch['x'].float().to(device)\n",
    "        y = batch['y'].float().to(device)\n",
    "        fname = batch['fname']\n",
    "        # print(x.shape, y.shape)\n",
    "        \n",
    "        yhat = F.sigmoid(net(x))\n",
    "        # print(yhat, y)\n",
    "        # print(f'epoch {epoch}, idx {idx}, yhat {yhat},y {y}')\n",
    "        # print(yhat,y)\n",
    "        loss = lossfn(yhat,y) #pytorch는 이 순서 / sklearn은 반대\n",
    "        # metric?\n",
    "        \n",
    "        train_accuracy.update(yhat.cpu().detach().round().to(torch.int64), y.cpu().detach().to(torch.int64)) #tensor\n",
    "            \n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.cpu().detach().numpy())  # 2개의 batch씩 학습되고 난 loss를 나열\n",
    "        \n",
    "    total_train_accuracy = train_accuracy.compute()\n",
    "    return np.mean(losses), total_train_accuracy\n",
    "\n",
    "def valid(loader):\n",
    "    losses = []\n",
    "    ys = []\n",
    "    yhats = []\n",
    "    net.eval()\n",
    "    for idx, batch in tqdm.tqdm(enumerate(loader), desc='valid', total=len(loader)):\n",
    "        x = batch['x'].float().to(device)\n",
    "        y = batch['y'].float().to(device)\n",
    "        fname = batch['fname']\n",
    "        # print(x.shape, y.shape)\n",
    "        \n",
    "        with torch.no_grad():  #valid에는 이게 꼭 있으면 좋다.\n",
    "            yhat = net(x)        \n",
    "            yhat = F.sigmoid(yhat)\n",
    "            # print(f'epoch {epoch}, idx {idx}, yhat {yhat},y {y}')\n",
    "            loss = lossfn(yhat,y) #pytorch는 이 순서 / sklearn은 반대\n",
    "            # metric?\n",
    "            valid_accuracy.update(yhat.cpu().detach().round().to(torch.int64), y.cpu().detach().to(torch.int64)) #tensor\n",
    "            #a= valid_accuracy(yhat, y) #tensor\n",
    "            #print(a,valid_accuracy.compute())\n",
    "            \n",
    "            ys.append(y.cpu().detach().numpy())\n",
    "            yhats.append(yhat.cpu().detach().numpy())\n",
    "        \n",
    "        losses.append(loss.cpu().detach().numpy())  # 2개의 batch씩 학습되고 난 loss를 나열\n",
    "        \n",
    "        # print(f'train_loss_seg:{loss_seg}',f'train_loss_reg:{loss_reg}'\n",
    "        # if idx == (len(loader) - 1) :\n",
    "            # print(f'valid_loss:{np.mean(losses)}')\n",
    "\n",
    "    total_valid_accuracy = valid_accuracy.compute()\n",
    "    valid_accuracy.reset()\n",
    "#    yhats = binarization(np.array(yhats)) 이게 왜 안될가?\n",
    "#    print(metrics(yhats,np.array(ys)))            \n",
    "    return np.mean(losses), total_valid_accuracy\n",
    "\n",
    "def test(loader):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    yhats = []\n",
    "    fnames = []\n",
    "    net.eval()\n",
    "    for idx, batch in tqdm.tqdm(enumerate(loader), desc='test', total=len(loader)):\n",
    "        x = batch['x'].float().to(device)\n",
    "        y = batch['y'].float().to(device)\n",
    "        fname = batch['fname']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            yhat = F.sigmoid(net(x))\n",
    "            loss = lossfn(yhat,y)\n",
    "\n",
    "        xs.extend(x.cpu().detach().numpy())\n",
    "        ys.extend(y.cpu().detach().numpy())\n",
    "        yhats.extend(yhat.cpu().detach().numpy())\n",
    "        fnames.extend(fname)\n",
    "\n",
    "    return xs, ys, yhats, fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d994d29-7813-4edb-90cd-6e4b90072dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train 날짜시간 폴더 만들기!\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "\n",
    "import os\n",
    "\n",
    "def createDirectory():\n",
    "    try:\n",
    "        if not os.path.exists(f'weights/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}'):\n",
    "            os.makedirs(f'weights/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}')\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "        \n",
    "createDirectory()\n",
    "\n",
    "# print(now.year, now.month, now.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7eee3-1494-4f74-81bc-e1736e63b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "plotlosses = PlotLosses()\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in tqdm.trange(epochs):\n",
    "    train_loss, train_acc = train(train_loader) \n",
    "    valid_loss, valid_acc = valid(valid_loader)\n",
    "\n",
    "    if epoch>20:\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)        \n",
    "\n",
    "    if valid_losses and np.min(valid_losses) == valid_loss:\n",
    "        torch.save(net, f'weights/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}_{epoch}.pt')  # save all models        \n",
    "        print(f'weights/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}_{epoch}: weight saved')\n",
    "        weight_name = f'weights/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}_{epoch}.pt'\n",
    "\n",
    "    plotlosses.update({\n",
    "        'loss': train_loss,\n",
    "        'val_loss': valid_loss,\n",
    "        'acc': train_acc,\n",
    "        'val_acc': valid_acc\n",
    "    })\n",
    "    plotlosses.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70171a63-c529-421f-bd33-371df1a3ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# print(train_losses)\n",
    "# print(valid_losses)\n",
    "df = pd.DataFrame({'train_losses': train_losses, 'valid_losses': valid_losses})\n",
    "df.to_csv('losses_log.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caef7b6-e087-4b60-bfe8-5fd7cdc46c5b",
   "metadata": {},
   "source": [
    "## 학습끝. Weight 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940d288-e3d6-4234-b181-896e654f563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_name = 'weights/binary_ENb3_Imb_NLB_best_20230130/binary_ENb3_Imb_NLB_best_20230130_379.pt'\n",
    "net = torch.load(weight_name)\n",
    "print(weight_name)\n",
    "\n",
    "experiment_name = 'binary_ENb3_Imb_NLB_best_20230130'\n",
    "weight_number = 379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e1d55-98db-4b7d-9066-4e54f948a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, yhats, fnames = test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0e9f7-752a-4ff3-9d3a-379faa43b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(xs), len(ys), len(yhats), len(fnames))\n",
    "print(type(xs), type(ys), type(yhats), type(fnames))\n",
    "print(type(xs[0]), type(ys[0]), type(yhats[0]), type(fnames[0]))\n",
    "print(xs[0].shape, ys[0].shape, yhats[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8f481-4f17-41e0-9596-46f18935f0d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a9cca3-1e90-45bf-b18c-ecb5b80f1653",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Confusion matrix 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b1198-bad6-4471-9326-a5a64465f96d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find unique values and counts\n",
    "unique_values, counts = np.unique(yhats, return_counts=True)\n",
    "\n",
    "# Iterate through the unique values and counts, and print them as pairs\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f'Value: {value}, Count: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdbb9e-7402-4e63-b4ed-b910e89d33f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(np.unique(ys))\n",
    "binarization = monai.transforms.AsDiscrete(threshold=0.5)\n",
    "yhats_binary50 = binarization(np.array(yhats))\n",
    "print(np.unique(yhats))\n",
    "print(np.unique(yhats_binary50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea662e-0833-41db-a306-db778d117342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "y_true = ys#.flatten()\n",
    "y_pred = yhats_binary50 #.flatten()\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['normal','abnormal'])\n",
    "disp.plot()\n",
    "# plt.savefig(f'weights/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}_{epoch}.png')\n",
    "plt.savefig(f'weights/{experiment_name}/{experiment_name}_{weight_number}_50.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb3d0f-07fd-4dd2-99f9-23a0c0f2cf50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84021f5-e762-4a88-83d2-32f4399b0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "# xs, ys, yhats, fnames = test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789eb1e8-0adc-4235-a493-64299062e0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(ys, yhats, pos_label=1)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                  estimator_name=f'{experiment_name}_{weight_number}')\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "fig, ax = plt.subplots() #2개만 받는다\n",
    "display.plot(ax=ax, figure=fig)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('False Positive Rate (1-Specificity)')\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.set_title('ROC Curve')\n",
    "\n",
    "# display.line_.set_color('blue')\n",
    "\n",
    "fig.savefig(f'weights/{experiment_name}/{experiment_name}_{weight_number}_roc_curve_example.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46063530-fdde-42d6-a461-aef6b65ff679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def youden_index(y_true, y_score):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    idx = np.argmax(tpr - fpr)\n",
    "    return thresholds[idx]\n",
    "\n",
    "youden = youden_index(ys, yhats)\n",
    "\n",
    "print(youden, type(youden))\n",
    "youden = np.asscalar(youden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f899606-a34c-42f3-930d-141029cfbc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ys:{np.unique(ys)}')\n",
    "# print(f'yhats:{np.unique(yhats)}')\n",
    "binarization = monai.transforms.AsDiscrete(threshold=youden)\n",
    "yhats_binary_youden = binarization((np.array(yhats)))\n",
    "print(np.unique(yhats_binary_youden))\n",
    "print(np.array(ys).shape, np.array(yhats_binary_youden).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d665c7-55c8-4fd6-a092-08df42a86aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "y_true = ys#.flatten()\n",
    "y_pred = yhats_binary_youden#.flatten()\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['normal','abnormal'])\n",
    "disp.plot()\n",
    "# plt.savefig(f'weights/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}/binary_ENb3_Imb_NLB_best_{now.year:02d}{now.month:02d}{now.day:02d}_{epoch}.png')\n",
    "plt.savefig(f'weights/{experiment_name}/{experiment_name}_{weight_number}_youden.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee57db-12eb-49ee-bc0a-627f30858d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e2cfb-3e3c-4a5b-a0a6-6def4fc32d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
